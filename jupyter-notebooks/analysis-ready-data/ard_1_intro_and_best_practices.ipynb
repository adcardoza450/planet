{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Ready Data Tutorial Part 1: Introduction and Best Practices\n",
    "\n",
    "Time-series analysis (e.g. change detection and trend detection) is a powerful application of satellite imagery. However, a great deal of processing is required to prepare imagery for analysis. Analysis Ready Data (ARD), preprocessed time-series stacks of overhead imagery, allow for time-series analysis without any additional processing of the imagery. See [Analysis Data Defined](https://medium.com/planet-stories/analysis-ready-data-defined-5694f6f48815) for an excellent introduction and discussion on ARD.\n",
    "\n",
    "This tutorial shows how [Planet APIs](https://developers.planet.com/docs/apis/) can simplify production of ARD by demonstrating best practices and then by walking through a real world use case. This tutorial is targeted to users who have little to no geospatial knowledge but have experience working with APIs. The goal of this tutorial is to teach the user the how and whys of using the Data and Orders APIs to create and interpret ARD for both use cases. This first part of the totorial focuses on best practices. The following part will focus on a real-world use case.\n",
    "\n",
    "\n",
    "## APIs\n",
    "\n",
    "The two Planet APIs that are used in creation of ARD are the [Data API](https://developers.planet.com/docs/data/) and the [Orders API](https://developers.planet.com/docs/orders/). The Data API is used to search for imagery according to certain criteria based on the use case, and the Orders API is used to process that imagery into ARD that can be fed directly into time-series analysis.\n",
    "\n",
    "### Data API\n",
    "\n",
    "The first step in using the Data API is identifying the search criteria. This is specifying answers to the following questions regarding the use case time-series analysis:\n",
    "* What is the time range?\n",
    "* What product item type is desired?\n",
    "* What is the area of interest (geographic region)?\n",
    "* What percentage of pixels need to be usable?\n",
    "* etc.\n",
    "\n",
    "While time range is likely pretty trivial to determine, product item, area of interest, and usable pixels may take a little bit of work. Let's dive into each further\n",
    "\n",
    "#### Product Item Type\n",
    "\n",
    "The [product item type](https://developers.planet.com/docs/data/items-assets/) refers to the sensor source (aka satellite type) and basic processing desired. This decision is highly dependent on application, as coverage, revisit rate, spectral bands, and resolution differ between products. A good overview of the products available in the Planet Data API is provided on the [Planet Imagery and Archive](https://www.planet.com/products/planet-imagery/) page (look for the link to product specs for details). For most frequent revisit rate, we will use the PS satellite. Experience has shown that customers most often use the scene (vs the orthotile) product. Therefore, this tutorial will focus on the `PSScene4Band` product.\n",
    "\n",
    "#### Area of Interest\n",
    "\n",
    "The area of interest is the geographic region for the analysis, given as GeoJSON. If you are familiar with JSON, the format of GeoJSON will likely be easy to grasp. According to <geojson.org>, \"GeoJSON is a format for encoding a variety of geographic data structures.\" The specific geographic data structure we are interested is a `Polygon`. The [GeoJSON wikipedia page](https://en.wikipedia.org/wiki/GeoJSON) gives some great examples of the data structures for the various GeoJSON data structures. \n",
    "\n",
    "Some care needs to be given to describing the [position](https://tools.ietf.org/html/rfc7946#section-3.1.1) for each coordinate in a GeoJSON geometry. Each position is specified as `(longitude, latitude)` or `(easting, northing)` (order really matters here!). Also, this may be a surprise, but the same point on earth can have different `(longitude, latitude)` values based on the spatial reference system. Basically, a spatial reference system describes where something is in the real world. But there are thousands of different spatial reference systems. These are separated into two categories: geographic and projected. Geographic coordinate systems model the earth as an ellipsoid (this model is called the `datum`) and describe positions on the surface of the earth (coordinates) in terms of the prime meridian and angle of measure. Projected coordinate systems take this a step further by projecting the geographic coordinates (quite three-dimensional) into two dimensions (the `projection`). Different projections preserve different properties, such as area, angles, or direction for north. There is a rich area of discovery, discussion, and even a little [teasing](https://xkcd.com/977/) (thanks xkcd!) in the world of spatial reference systems.\n",
    "\n",
    "GeoJSON only supports one spatial reference system, [WGS84](https://spatialreference.org/ref/epsg/wgs-84/). This is a geographic coordinate system describing locations in latitude, longitude. However, many web mapping applications use the Web Mercator projected coordinate system to to describe locations. Confusingly, Web Mercator also describes locations in latitude, longitude. But a `(longitude, latitude)` GeoJSON position given in Web Mercator will **not** end up where you expect if it is not first projected into WGS84. The easiest way to define an aoi that is described in WGS84 is to draw it in [Planet Explorer](https://www.planet.com/explorer) and click the little button \"Download AOI as GeoJSON.\"\n",
    "\n",
    "#### Usable Pixels\n",
    "\n",
    "Taking pictures of the earth from space ain't easy. There are a lot of steps and a lot of things have to go right to get a clear shot. Therefore, not every pixel in every image taken from space is useful for analysis. For one, images taken from space are projected into a spatial reference system (discussed briefly above), a process that introduces some NoData (aka outside of the image footprint) pixels into the resulting image. Additionally, clouds cover a great deal of the earth and create cloudy pixels when imaged. While some applications can use cloudy pixels, others cannot. Therefore, the type of pixels that are determined to be 'usable' are often application-specific. To support definition of usable pixels, and filtering based on that definition, Planet provides Usable Data Masks along with Usable Data entries in the imagery metadata. For more details on the Usable Data Mask, check out [Clear for Analysis with Planetâ€™s New Usable Data Masks](https://www.planet.com/pulse/planets-new-usable-data-masks/). You can also find great examples for working with Usable Data Masks in the [UDM2 notebooks](https://github.com/planetlabs/notebooks/tree/master/jupyter-notebooks/udm2). For more information on the Usable Data metadata entries, see [Usable Data in Planet imagery](https://developers.planet.com/planetschool/usable-data-in-planet-imagery/).\n",
    "\n",
    "### Orders API\n",
    "\n",
    "The core decision around using the orders api is which [product bundle](https://developers.planet.com/docs/orders/product-bundles-reference/) to use. This is the starting point for all processing and there are a lot of options. Once the product is determined, the processing steps (aka tools and toolchains) are defined. Finally, the logistics of the delivery of the imagery are ironed out.\n",
    "\n",
    "#### Product Bundle\n",
    "\n",
    "To enable time-series analysis, ARD imagery must be processed so that imagery is consistant across days, months, and possibly years. This means correcting for differences in camera sensitivities, the relative location of the sun, and the atmospheric conditions. The Analytic Radiance (`analytic`) product bundle provides imagery corrected for difference in camera sensitivities and location of the sun (as radiance) and can also remove the effect of the sun's spectrum by applying the `reflectanceCoefficient` value given in the imagery metadata. However, the Analytic Surface Reflectance (`analytic_sr`) product bundle removes the effect of atmospheric conditions while also converting to reflectance. Therefore, the Analytic Surface Reflectance is the ideal product for ARD and the one we will use here.\n",
    "\n",
    "#### Tools and Toolchains\n",
    "\n",
    "The [Tools and Toolchains](https://developers.planet.com/docs/orders/tools-toolchains/) functionality in the Orders API are the key to seamlessly creating ARD. Through the API, one can define the pre-processing steps for the data before it is delivered. Given proper definition of the tools and toolchains, data that is delivered is analysis-ready.\n",
    "\n",
    "#### Delivery\n",
    "\n",
    "There are a few options for delivery that cater to different use cases. Imagery can be downloaded directly or delivered to [cloud storage](https://developers.planet.com/docs/orders/ordering-delivery/#delivery-to-cloud-storage). When imagery is downloaded, the user can poll for when the order is ready or notifications ([e-mail](https://developers.planet.com/docs/orders/ordering-delivery/#email-notification) or [webhooks](https://developers.planet.com/docs/orders/ordering-delivery/#using-webhooks)) can be used. Additionally, the imagery can be delivered as a [zip archive](https://developers.planet.com/docs/orders/ordering-delivery/#zipping-results).\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "Now that we have a basic understanding of the Data and Orders APIs, let's put them to use creating ARD. This will be a demonstration of best practices.\n",
    "\n",
    "For this tutorial we will use the [planet python client](https://github.com/planetlabs/planet-client-python) ([documentation](https://planetlabs.github.io/planet-client-python/index.html)). This client simplifies interactions with the various Planet APIs and also includes a command-line interface.\n",
    "\n",
    "The first step in using the planet python client is initializing the client with the user API key. Each user (or organization) has their own unique API key. Information on finding the API key and running Docker so that the API key is available in the notebooks is given in this repository's [README](https://github.com/planetlabs/notebooks#install-and-use-these-notebooks).\n",
    "\n",
    "The next step is building functionality for searching the Data API with the client. This consists of building the search query and then running the search. In building this functionality, we will use test information for data ranges and AOIs to test the functionality, but we will build the functions so those pieces can be changed by the end user. Fo this step we will demonstrate use of  both the python api and the cli.\n",
    "\n",
    "The third step is building functionality for processing and delivery with the Orders API. At the time of the creation of this tutorial (June 2019), the Orders API functionality best supported (e.g. [documented](https://planetlabs.github.io/planet-client-python/cli/examples.html#orders-examples)) in the command-line interface (CLI). Therefore, we will use the client CLI for this portion of the tutorial.\n",
    "\n",
    "Finally, once we have downloaded the order, we will unzip it and visualize the resulting imagery.\n",
    "\n",
    "\n",
    "To summarize, these are the steps:\n",
    "1. [Initialize API client](#Step-1:-Initialize-API-client)\n",
    "1. [Search Data API](#Step-2:-Search-Data-API)\n",
    "1. [Submit Order](#Step-3:-Submit-Order)\n",
    "1. [Download Order](#Step-4:-Download-Order)\n",
    "1. [Unzip and View Order](#Step-5:-Unzip-and-View-Order)\n",
    "\n",
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import planet\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from planet import Auth\n",
    "from planet import Session, DataClient, OrdersClient, data_filter, order_request\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from shapely.geometry import MultiPolygon, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if your Planet API Key is not set as an environment variable, you can paste it below\n",
    "API_KEY = os.getenv('PL_API_KEY', 'PASTE_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Auth.from_key(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Search Data API\n",
    "\n",
    "The goal of this step is to get the scene ids that meet the search criteria for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "\n",
    "# iowa crops aoi\n",
    "test_aoi_geom = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-93.299129, 42.699599],\n",
    "            [-93.299674, 42.812757],\n",
    "            [-93.288436, 42.861921],\n",
    "            [-93.265332, 42.924817],\n",
    "            [-92.993873, 42.925124],\n",
    "            [-92.993888, 42.773637],\n",
    "            [-92.998396, 42.754529],\n",
    "            [-93.019154, 42.699988],\n",
    "            [-93.299129, 42.699599]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "### Let's search:\n",
    "# for the geometry above\n",
    "# Date Range: Jan 1st - Jan 31st 2020\n",
    "# Clear Percent: 90% or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an API Request from the search specifications\n",
    "\n",
    "item_type = ['PSScene']\n",
    "\n",
    "geom_filter = data_filter.geometry_filter(test_aoi_geom)\n",
    "clear_percent_filter = data_filter.range_filter('clear_percent', None, None, 90)\n",
    "date_range_filter = data_filter.date_range_filter(\"acquired\", datetime(month=1, day=1, year=2020), datetime(month=1, day=31, year=2020))\n",
    "\n",
    "combined_filter = data_filter.and_filter([geom_filter, clear_percent_filter, date_range_filter])\n",
    "\n",
    "async with Session() as sess:\n",
    "    cl = DataClient(sess)\n",
    "    request = await cl.create_search(name='temp_search',search_filter=combined_filter, item_types=item_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our search request.\n",
    "# Note: This is just the request's structure, the search hasn't been implemented yet\n",
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search the Data API\n",
    "async with Session() as sess:\n",
    "    cl = DataClient(sess)\n",
    "    items = await cl.run_search(search_id=request['id'])\n",
    "    item_list = [i async for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out an item just for fun\n",
    "print(item_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a scene footprint\n",
    "footprints = [shape(i['geometry']) for i in item_list]\n",
    "footprints[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize subset of footprints and aoi\n",
    "MultiPolygon([shape(test_aoi_geom), *footprints[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the footprints (rectangles) do not exactly match the AOI. Indeed, none of them cover the AOI. We don't care about pixels outside of the AOI, so we are going to want to clip the imagery to the AOI (to remove pixels outside the AOI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Submit Order\n",
    "\n",
    "Now that we have the scene ids, we can create the order. The output of this step is a single zip file that contains all of the scenes that meet our criteria.\n",
    "\n",
    "Because this is a demo, we don't download all of the scene ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with just a subset of the items in the interest of bandwidth\n",
    "test_items = item_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to item ids\n",
    "ids = [i['id'] for i in test_items]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the psscene4band surface reflectance product\n",
    "# make sure to get the *_udm2 bundle so you get the udm2 product\n",
    "# note: capitalization really matters in item_type when using planet client orders api\n",
    "item_type = 'PSScene'\n",
    "bundle = 'analytic_sr_udm2'\n",
    "# specify a name\n",
    "name = 'tutorial_order'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1: Build Orders Toolchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify tools\n",
    "\n",
    "# clip to AOI\n",
    "clip_tool = {'clip': {'aoi': test_aoi_geom}}\n",
    "\n",
    "# convert to NDVI\n",
    "bandmath_tool = {'bandmath': {\n",
    "    \"pixel_type\": \"32R\",\n",
    "    \"b1\": \"(b4 - b3) / (b4+b3)\"\n",
    "}}\n",
    "\n",
    "tools = [clip_tool, bandmath_tool]\n",
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can build our order request using the Python SDK's order_request feature\n",
    "\n",
    "products = [order_request.product(ids, bundle, item_type)]\n",
    "\n",
    "request = order_request.build_request('test_order_sdk_method_2', products=products, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our request\n",
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the request as a json file as well\n",
    "with open('json_request.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2: Submit Order\n",
    "\n",
    "###### Option 1: Client Python API\n",
    "\n",
    "The first option for submitting an order is using the planet client python api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the order\n",
    "async with Session() as sess:\n",
    "    cl = OrdersClient(sess)\n",
    "    order = await cl.create_order(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the order info\n",
    "order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Option 2: Client CLI\n",
    "\n",
    "In some instances, using the CLI to submit orders may be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!planet orders create 'json_request.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we see our Order ID above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Download Order\n",
    "\n",
    "To download the order from the orders api, we will use the planet python client CLI. It would be nice to use the python client python api for this step but, as of the writing of this tutorial, support for the orders api in the planet client python api has been [confusing](https://github.com/planetlabs/planet-client-python/issues/217). The CLI download status output is also [confusing and possibly wrong](https://github.com/planetlabs/planet-client-python/issues/218) but the CLI does easily and successfully download the order.\n",
    "\n",
    "When we download an order, we always get a `manifest.json` file. Therefore, while we only ordered one file (an order zip), we will download two files. The manifest is [very useful](https://developers.planet.com/docs/orders/ordering-delivery/#why-you-should-depend-on-the-manifest-file) and we will use it to locate the zip file we ordered and downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1: Run Download\n",
    "\n",
    "For this step we will use the Orders CLI because it is the easiest and best supported way to download via the planet client for now.\n",
    "\n",
    "One thing to watch out for: if you have already downloaded an order, it won't be available for re-download. You will have to resubmit the order and get a new order id. What is tricky is that the manifest is still downloaded, but the other files are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data_dir = os.path.join('data', 'demo')\n",
    "\n",
    "# make the download directory if it doesn't exist\n",
    "Path(demo_data_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we will use the Order ID from our API Client request\n",
    "\n",
    "order_id = order[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, we will make sure the order has reached a downloadable state. This may take several minutes.\n",
    "!planet orders wait $order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!planet orders download $order_id --directory $demo_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.2: Get Downloaded File Location(s)\n",
    "\n",
    "We use the downloaded order manifest to find the downloaded file locations. The manifest is saved in the download directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls data/demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_download_locations(download_dir):\n",
    "    manifest_file = os.path.join(download_dir, order_id, 'manifest.json')\n",
    "    with open(manifest_file, 'r') as src:\n",
    "        manifest = json.load(src)\n",
    "    \n",
    "    # uncomment to see the manifest\n",
    "    # pprint(manifest)\n",
    "        \n",
    "    locations = [os.path.join(download_dir, order_id, f['path'])\n",
    "                 for f in manifest['files']]\n",
    "    return locations\n",
    "\n",
    "locations = get_download_locations(demo_data_dir)\n",
    "pprint(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 5: Unzip and View Order\n",
    "\n",
    "In this step we will simply unzip the order and view the downloaded images and their usable data masks.\n",
    "\n",
    "##### 5.1: Unzip Order\n",
    "\n",
    "We will unzip the order into a directory named after the file, then we will find the downloaded files (they are in a `files` subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(filename):\n",
    "    location = Path(filename)\n",
    "    \n",
    "    zipdir = location.parent / location.stem\n",
    "    with ZipFile(location) as myzip:\n",
    "        myzip.extractall(zipdir)\n",
    "    return zipdir\n",
    "\n",
    "zipdir = unzip(locations[0])\n",
    "zipdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unzipped_files(zipdir):\n",
    "    filedir = zipdir / 'files'\n",
    "    filenames = os.listdir(filedir)\n",
    "    return [filedir / f for f in filenames]\n",
    "\n",
    "file_paths = get_unzipped_files(zipdir)\n",
    "pprint(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2: Visualize Images\n",
    "\n",
    "In this section we will find the image files and their associated UDMs and we will visualize them.\n",
    "\n",
    "The first band of the UDM2 file is the clear/not-clear band. 0: not-clear, 1: clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_udm_files(file_paths):\n",
    "    files = [str(p) for p in file_paths]\n",
    "    \n",
    "    # the image files are tiffs and are identified with '_SR_' in the name\n",
    "    img_id = '_AnalyticMS_SR_'\n",
    "    imgfiles = [f for f in files\n",
    "                if f.endswith('.tif') and img_id in f]\n",
    "    \n",
    "    # get associated udm files for image files\n",
    "    # each image has a unique id at the beginning of the name\n",
    "    imgroots = [str(f).split(img_id)[0] for f in imgfiles]\n",
    "    \n",
    "    # the udm files are identified with '_udm2' in the name\n",
    "    udmfiles = [next(f for f in files if f.startswith(r + '_udm2'))\n",
    "                for r in imgroots]\n",
    "    \n",
    "    return imgfiles, udmfiles\n",
    "\n",
    "imgfiles, udmfiles = get_image_and_udm_files(file_paths)\n",
    "pprint(imgfiles)\n",
    "pprint(udmfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read UDM2 file\n",
    "def read_notclear(udm2_filename):\n",
    "    with rasterio.open(udm2_filename) as img:\n",
    "        # the first band is the clear/not clear band\n",
    "        mask=img.read(1)\n",
    "        not_clear = mask == 0\n",
    "        return not_clear\n",
    "    \n",
    "udmfile = udmfiles[0]\n",
    "not_clear = read_notclear(udmfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is an issue where some udms aren't the same size as the images\n",
    "# to deal with this just cut off any trailing rows/columns\n",
    "# this isn't ideal as it can result in up to one pixel shift in x or y direction\n",
    "def crop(img, shape):\n",
    "    return img[:shape[0], :shape[1]]\n",
    "\n",
    "def read_ndvi(img_filename, not_clear):\n",
    "    with rasterio.open(imgfile) as img:\n",
    "        # ndvi is a single-band image\n",
    "        band = img.read(1)\n",
    "        \n",
    "        # crop image and mask to same size\n",
    "        img_shape = min(band.shape, not_clear.shape)\n",
    "        ndvi = np.ma.array(crop(band, img_shape), mask=crop(not_clear, img_shape))\n",
    "    return ndvi\n",
    "    \n",
    "imgfile = imgfiles[0]\n",
    "ndvi = read_ndvi(imgfile, not_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up NDVI visualization\n",
    "# copied from:  https://stackoverflow.com/a/48598564\n",
    "\n",
    "\"\"\"\n",
    "The NDVI values will range from -1 to 1. You want to use a diverging color scheme to visualize the data,\n",
    "and you want to center the colorbar at a defined midpoint. The class below allows you to normalize the colorbar.\n",
    "\"\"\"\n",
    "class MidpointNormalize(colors.Normalize):\n",
    "    \"\"\"\n",
    "    Normalise the colorbar so that diverging bars work there way either side from a prescribed midpoint value)\n",
    "    e.g. im=ax1.imshow(array, norm=MidpointNormalize(midpoint=0.,vmin=-100, vmax=100))\n",
    "    Credit: Joe Kington, http://chris35wills.github.io/matplotlib_diverging_colorbar/\n",
    "    Credit: https://stackoverflow.com/a/48598564\n",
    "    \"\"\"\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "    \n",
    "    def __call__(self, value, clip=None):    \n",
    "        # Note that I'm ignoring clipping and other edge cases here.\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.array(np.interp(value, x, y), mask=result.mask, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ndvi(ndvi):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # diverging color scheme chosen from https://matplotlib.org/users/colormaps.html\n",
    "    cmap = plt.cm.RdYlGn \n",
    "\n",
    "    mmin = np.nanmin(ndvi)\n",
    "    mmax = np.nanmax(ndvi)\n",
    "\n",
    "    mid = 0\n",
    "\n",
    "    cax = ax.imshow(ndvi, cmap=cmap, clim=(mmin, mmax),\n",
    "                    norm=MidpointNormalize(midpoint=mid,vmin=mmin, vmax=mmax))\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Normalized Difference Vegetation Index', fontsize=18, fontweight='bold')\n",
    "\n",
    "    cbar = fig.colorbar(cax, orientation='horizontal', shrink=0.65)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for imgfile, udmfile in zip(imgfiles, udmfiles):\n",
    "    show_ndvi(read_ndvi(imgfile, read_notclear(udmfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we got some beautiful NDVI images down! Notice on the second image that the clouds are masked out thanks to the UDM2 band."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
