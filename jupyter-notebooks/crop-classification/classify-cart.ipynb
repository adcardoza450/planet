{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Type Classification: CART\n",
    "\n",
    "Our aim in this notebook is to classify crop type using PlanetScope 4-band Orthotiles. The crop types of particular interest are corn and soybeans.\n",
    "\n",
    "[CART](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) is a decision tree algorithm that has shown great promise for classification of remotely sensed imagery. We will use this algorithm to classify crop type.\n",
    "\n",
    "In this notebook, we will focus on using the PlanetScope imagery 4 bands as well as NDVI calculation as the features that are fed into the CART algorithm. We will train on one PS Orthotile and validate on another PS Orthotile.\n",
    "\n",
    "### Outline\n",
    "1. Identify datasets (PS Orthotiles and gold standard dataset for train and test)\n",
    "1. Train classifier\n",
    "1. Test classifier\n",
    "\n",
    "\n",
    "### Gold Standard Dataset\n",
    "\n",
    "The [USDA 2016 Crop Data Layer](https://www.nass.usda.gov/Research_and_Science/Cropland/Release/index.php) (CDL) provides a national crop type dataset. This dataset was build using Landsat 8, DMC Deimos-1, and UK 2 satellite imagery ([src](https://www.nass.usda.gov/Research_and_Science/Cropland/sarsfaqs2.php#Section3_17.0)), using supervised classification (decision trees) based on ground truth from the Farm Service Agency (FSA) Common Land Unit (CLU) Program. Since this is a derived dataset, it isn't a ground truth dataset but it is known to be quite accurate so can be used as our gold standard dataset. This dataset is provided as a georegistered raster (geoTIFF).\n",
    "\n",
    "Since corn and soybeans are the primary crops grown in Iowa, we will focus our analysis in that state. The [metadata](https://www.nass.usda.gov/Research_and_Science/Cropland/metadata/metadata_ia16.htm) provided for the Iowa 2016 CDL indicates that it's accuracy is 96.4% and that corn (categorization code 1) and soybeans (categorization code 5) are indeed the primary crop types in the state.\n",
    "\n",
    "### Growth Season\n",
    "\n",
    "[Iowa 1997 statistics](https://usda.mannlib.cornell.edu/usda/nass/planting/uph97.pdf): corn planting ended June 3, harvesting began Sept 17 and soybean planting ended June 17 and harvesting began Sept 21. Therefore, imagery should be limited to between June 3 and Sept 17, 2016.\n",
    "\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "from subprocess import check_output, STDOUT, CalledProcessError\n",
    "import tempfile\n",
    "from xml.dom import minidom\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Datasets\n",
    "\n",
    "### PS Orthotoile\n",
    "\n",
    "Get a PS Orthotile that has majority coverage over the grid, is from a SSO satellite, is free from clouds, and was taken between June 3 and Sept 17, 2016.\n",
    "\n",
    "Using [planet explorer](https://www.planet.com/explorer/), [210879_1558814_2016-07-25_0e16](https://api.planet.com/data/v1/item-types/PSOrthoTile/items/210879_1558814_2016-07-25_0e16/thumb) was identified as a good candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Scene and Supporting Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment below to learn more about PSOrthotile 210879_1558814_2016-07-25_0e16\n",
    "\n",
    "# !planet data search --item-type PSOrthoTile --string-in id 210879_1558814_2016-07-25_0e16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment below to download scene and supporting files to local folder\n",
    "\n",
    "# !mkdir -p data/cart/210879_1558814_2016-07-25_0e16\n",
    "# !planet data download --item-type PSOrthoTile --dest data/cart/210879_1558814_2016-07-25_0e16 \\\n",
    "#     --asset-type analytic,analytic_xml,udm --string-in id 210879_1558814_2016-07-25_0e16\n",
    "# !ls -l --block-size=M data/cart/210879_1558814_2016-07-25_0e16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml\n"
     ]
    }
   ],
   "source": [
    "# define data file filenames and ensure they exist\n",
    "train_folder = os.path.join('data', 'cart', '210879_1558814_2016-07-25_0e16')\n",
    "\n",
    "train_files = {\n",
    "    'scene': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif'),\n",
    "    'metadata': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml'),\n",
    "    'udm': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif'),\n",
    "}\n",
    "\n",
    "for filename in train_files.values():\n",
    "    print(filename)\n",
    "    assert os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 Iowa CDL\n",
    "\n",
    "Crop type information is provided for Iowa for 2016 from the [USDA 2016 Crop Data Layer](https://www.nass.usda.gov/Research_and_Science/Cropland/Release/index.php) (CDL). We are specifically interested in the CDL for Iowa.\n",
    "\n",
    "The [metadata](https://www.nass.usda.gov/Research_and_Science/Cropland/metadata/metadata_ia16.htm) for the Iowa CDL provides the conversion between tif class values and crop type. The values along with the number of pixels associated with those values can be found in the Data_Quality_Information section of the metadata. The value for corn is 1 and the value for soybeans is 5. Note that there are some values associated with double crops that include corn or soybeans but the pixel count for those categories is zero so they can be disregarded.\n",
    "\n",
    "In this section, we prepare the Iowa CDL tif for processing, load a block of the tif that matches the block loaded from the PS Orthotile, and then look at the unique values present in the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain and Prepare Data File\n",
    "\n",
    "In this section, we download the 2016 Iowa CDL and crop it to the PS Orthotile footprint.\n",
    "\n",
    "2016 CDL for Iowa is obtained through the [CropScape](https://nassgeodata.gmu.edu/CropScape/) site. On that site, ensure the layer is 'CropLand Data Layers -> 2016', then click the icon for 'Define Area of Interest by State...' (looks like an outline of the US with the US flag design). Under 'Select a State', select Iowa and click 'Submit.' Next, click the icon for 'Download Defined Area of Interest Data.' In the popup, ensure the 'CDL' tab is open and '2016' is selected, then click 'Select.' Another popup should appear with 'Please Wait...' then after a while, the popup should be replaced by a popup that says 'Download files from server'. Click 'Download' and after a bit the download should begin.\n",
    "\n",
    "Unzip the downloaded folder and move the tif, `CDL_2016_19.tif` to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CDL_2016_19.tif\n"
     ]
    }
   ],
   "source": [
    "# ensure the tif file is present\n",
    "cdl_full = os.path.join('data', 'CDL_2016_19.tif')\n",
    "print(cdl_full)\n",
    "assert os.path.isfile(cdl_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop and Project CDL\n",
    "\n",
    "We project, resample, and crop the CDL to match the Orthotile and save in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions: crop, resample, and project an image\n",
    "\n",
    "# These use gdalwarp. for a description of gdalwarp command line options, see:\n",
    "# http://www.gdal.org/gdalwarp.html\n",
    "\n",
    "def gdalwarp_project_options(src_crs, dst_crs):\n",
    "    src_proj = _crs_to_string(src_crs)\n",
    "    dst_proj = _crs_to_string(dst_crs)\n",
    "    return ['-s_srs', src_crs.to_string(), '-t_srs', dst_crs.to_string()]\n",
    "\n",
    "def gdalwarp_crop_options(bounds, crs):\n",
    "    xmin, ymin, xmax, ymax = [str(b) for b in bounds]\n",
    "    dst_proj = _crs_to_string(crs)\n",
    "    # -te xmin ymin xmax ymax\n",
    "    return ['-te', xmin, ymin, xmax, ymax]\n",
    "\n",
    "def gdalwarp_resample_options(width, height, technique='near'):\n",
    "    # for technique options, see: http://www.gdal.org/gdalwarp.html\n",
    "    return ['-ts', width, height, '-r', technique]\n",
    "\n",
    "def gdalwarp(input_filename, output_filename, options):\n",
    "    commands = _gdalwarp_commands(input_filename, output_filename, options)\n",
    "\n",
    "    # print error if one is encountered\n",
    "    # https://stackoverflow.com/questions/29580663/save-error-message-of-subprocess-command\n",
    "    try:\n",
    "        output = check_output(commands, stderr=STDOUT)\n",
    "    except CalledProcessError as exc:\n",
    "        print(exc.output)\n",
    "\n",
    "def _gdalwarp_commands(input_filename, output_filename, options):\n",
    "    commands = ['gdalwarp'] + options + \\\n",
    "               ['-overwrite',\n",
    "                input_filename,\n",
    "                output_filename]\n",
    "    print(' '.join(commands))\n",
    "    return commands\n",
    "\n",
    "def _test():\n",
    "    TEST_DST_SCENE = train_files['scene']\n",
    "    TEST_SRC_SCENE = cdl_full\n",
    "\n",
    "    with rasterio.open(TEST_DST_SCENE, 'r') as dst:\n",
    "        with rasterio.open(TEST_SRC_SCENE, 'r') as src:\n",
    "            print(_gdalwarp_project_options(src.crs, dst.crs))\n",
    "\n",
    "            print(_gdalwarp_crop_options(dst.bounds, dst.crs))\n",
    "\n",
    "            print(_gdalwarp_resample_options(dst.width, dst.height))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_cdl_image(cdl_filename, dst_filename, out_filename):\n",
    "    '''Project, crop, and resample cdl image to match dst_filename image.'''\n",
    "    \n",
    "    with rasterio.open(cdl_filename, 'r') as src:\n",
    "        with rasterio.open(dst_filename, 'r') as dst:\n",
    "            # project\n",
    "            src_crs = _add_nad_datum(src.crs) # Manually add NAD83 datum\n",
    "            proj_options = _gdalwarp_project_options(src_crs, dst.crs)\n",
    "\n",
    "            # crop\n",
    "            crop_options = _gdalwarp_crop_options(dst.bounds, dst.crs)\n",
    "\n",
    "            # resample\n",
    "            width, height = dst.shape\n",
    "            resample_options = _gdalwarp_resample_options(str(width), str(height), 'near')\n",
    "\n",
    "            options = proj_options + crop_options + resample_options\n",
    "            \n",
    "            # run gdalwarp\n",
    "            gdalwarp(cdl_filename, out_filename, options)\n",
    "\n",
    "def _add_nad_datum(crs):\n",
    "    '''Rasterio is not reading the datum for the CDL image so add it manually'''\n",
    "    crs.update({'datum': 'NAD83'})\n",
    "    return crs\n",
    "\n",
    "def _test(delete=True):\n",
    "    TEST_DST_SCENE = train_files['scene']\n",
    "    TEST_SRC_SCENE = cdl_full\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.tif', delete=delete, dir='.') as out_file:\n",
    "        # create output\n",
    "        prepare_cdl_image(TEST_SRC_SCENE, TEST_DST_SCENE, out_file.name)\n",
    "\n",
    "        # check output\n",
    "        with rasterio.open(TEST_DST_SCENE, 'r') as dst:\n",
    "            with rasterio.open(out_file.name, 'r') as src:\n",
    "                assert dst.crs == src.crs, '{} != {}'.format(src.crs, dst.crs)\n",
    "                assert dst.bounds == src.bounds\n",
    "                assert dst.shape == src.shape\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cart/210879_1558814_2016-07-25_0e16/CDL_2016_19_prep.tif\n"
     ]
    }
   ],
   "source": [
    "# create train dataset gold image from CDL image\n",
    "prep_filename = os.path.join(train_folder, 'CDL_2016_19_prep.tif')\n",
    "\n",
    "# only generate output if it doesn't already exist\n",
    "if not os.path.isfile(prep_filename):\n",
    "    prepare_cdl_image(cdl_full, train_files['scene'], prep_filename)\n",
    "    \n",
    "train_files['gold'] = prep_filename\n",
    "print(train_files['gold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier\n",
    "\n",
    "### PS Orthotile to features\n",
    "\n",
    "In this section we generate classification features from the 4 bands of the PS Orthotile and NDVI calculated from the Orthotile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for reading a window of an image\n",
    "\n",
    "def _read_window(filename, window):\n",
    "    with rasterio.open(filename, 'r') as src:\n",
    "        return src.read(window=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert UDM to mask\n",
    "\n",
    "We convert the UDM file to a boolean mask that can be used to generate masked band arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_udm_mask(udm_filename, window=None):\n",
    "    udm_array = _read_window(udm_filename, window)\n",
    "    return _udm_to_mask(udm_array)\n",
    "    \n",
    "def _udm_to_mask(udm_array):\n",
    "    '''Convert UDM bit-encoded array to boolean mask.\n",
    "    \n",
    "    The description for the information encoded in the bits in the UDM is given here:\n",
    "    https://www.planet.com/docs/spec-sheets/sat-imagery/\n",
    "    \n",
    "    Pixels with no issues have all bits set to 0, therefore their values are zero.\n",
    "    \n",
    "    We mask all pixels with any issues.\n",
    "    '''\n",
    "    return udm_array != 0\n",
    "\n",
    "def _test(): \n",
    "    udm_mask = load_udm_mask(train_files['udm'], window=((0,1000),(0,1000)))\n",
    "    print(udm_mask.shape)\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Mask Bands\n",
    "\n",
    "We load the orthotile bands (optionally applying a window) and mask them using the udm mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NamedBands = namedtuple('NamedBands', 'b, g, r, nir')\n",
    "\n",
    "def load_masked_bands(scene_filename, udm_filename, window=None):\n",
    "    \"\"\"Loads a 4-band BGRNir Planet Image file as a list of masked bands.\n",
    "    \n",
    "    The masked bands share the same mask, so editing one band mask will\n",
    "    edit them all.\n",
    "    \"\"\"\n",
    "    bands = load_bands(scene_filename, window=window)\n",
    "    mask = load_udm_mask(udm_filename, window=window)\n",
    "    return NamedBands(*[np.ma.array(b, mask=mask)\n",
    "                        for b in bands])\n",
    "\n",
    "def load_bands(filename, window=None):      \n",
    "    b, g, r, nir = _read_window(filename, window=window)\n",
    "    return NamedBands(b=b, g=g, r=r, nir=nir)\n",
    "\n",
    "def get_mask(bands):\n",
    "    return bands[0].mask\n",
    "\n",
    "\n",
    "def check_mask(bands):\n",
    "    band_mask = get_mask(bands)\n",
    "    return '{}/{} ({}%) masked'.format(band_mask.sum(), band_mask.size, (100.0*band_mask.sum())/band_mask.size)\n",
    "\n",
    "\n",
    "def _test():\n",
    "    print(check_mask(load_masked_bands(train_files['scene'],\n",
    "                                       train_files['udm'],\n",
    "                                       window=((500,1500),(500,1500)))))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Reflectance\n",
    "\n",
    "The PSOrthotiles provide the top of atmosphere radiance values for each band. It is preferred to work with top of atmosphere reflectance, as this is more consistent across time. Therefore, we convert each band to reflectance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: converting an image to reflectance\n",
    "\n",
    "NamedCoefficients = namedtuple('NamedCoefficients', 'b, g, r, nir')\n",
    "\n",
    "def read_refl_coefficients(metadata_filename):\n",
    "    # https://github.com/planetlabs/notebooks/blob/master/jupyter-notebooks/ndvi/ndvi_planetscope.ipynb\n",
    "    xmldoc = minidom.parse(metadata_filename)\n",
    "    nodes = xmldoc.getElementsByTagName(\"ps:bandSpecificMetadata\")\n",
    "\n",
    "    # XML parser refers to bands by numbers 1-4\n",
    "    coeffs = {}\n",
    "    for node in nodes:\n",
    "        bn = node.getElementsByTagName(\"ps:bandNumber\")[0].firstChild.data\n",
    "        if bn in ['1', '2', '3', '4']:\n",
    "            i = int(bn)\n",
    "            value = node.getElementsByTagName(\"ps:reflectanceCoefficient\")[0].firstChild.data\n",
    "            coeffs[i] = float(value)\n",
    "    return NamedCoefficients(b=coeffs[1],\n",
    "                             g=coeffs[2],\n",
    "                             r=coeffs[3],\n",
    "                             nir=coeffs[4])\n",
    "\n",
    "def _apply_coeffs(bands, coeffs):\n",
    "    return NamedBands(*[b.astype(float)*c for b,c in zip(bands, coeffs)])\n",
    "\n",
    "\n",
    "def _test():\n",
    "    print(read_refl_coefficients(train_files['metadata']))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reflectance_bands(scene_file, udm_file, metadata_file, window=None):\n",
    "    return _to_reflectance(load_masked_bands(scene_file, udm_file, window=window),\n",
    "                                metadata_file)\n",
    "\n",
    "def _to_reflectance(bands, metadata_filename):\n",
    "    coeffs = read_refl_coefficients(metadata_filename)\n",
    "    return _apply_coeffs(bands, coeffs)\n",
    "\n",
    "def _test():\n",
    "    bands = load_reflectance_bands(train_files['scene'],\n",
    "                                   train_files['udm'],\n",
    "                                   train_files['metadata'],\n",
    "                                   window=((500,1500),(500,1500)))\n",
    "    print([b.mean() for b in bands])\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Image\n",
    "\n",
    "To ensure everything loaded correctly, we display the image with and without the mask converted to the alpha channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions: displaying an image\n",
    "\n",
    "def _linear_scale(ndarray, old_min, old_max, new_min, new_max):\n",
    "    \"\"\"Linear scale from old_min to new_min, old_max to new_max.\n",
    "    \n",
    "    Values below min/max are allowed in input and output.\n",
    "    Min/Max values are two data points that are used in the linear scaling.\n",
    "    \"\"\"\n",
    "    #https://en.wikipedia.org/wiki/Normalization_(image_processing)\n",
    "    return (ndarray - old_min)*(new_max - new_min)/(old_max - old_min) + new_min\n",
    "# print(linear_scale(np.array([1,2,10,100,256,2560, 2660]), 2, 2560, 0, 256))\n",
    "\n",
    "def _mask_to_alpha(bands):\n",
    "    band = np.atleast_3d(bands[0])\n",
    "    alpha = np.zeros_like(band)\n",
    "    alpha[~band.mask] = 1\n",
    "    return alpha\n",
    "\n",
    "def _add_alpha_mask(bands):\n",
    "    return np.dstack([bands, _mask_to_alpha(bands)])\n",
    "\n",
    "def scale_bands(band_stack):\n",
    "    old_min = np.percentile(band_stack, 2)\n",
    "    old_max = np.percentile(band_stack, 98)\n",
    "    old_min = band_stack.min()\n",
    "    old_max = band_stack.max()\n",
    "\n",
    "    new_min = 0\n",
    "    new_max = 1\n",
    "    scaled = [np.clip(_linear_scale(b.astype(np.float),\n",
    "                                    old_min, old_max,\n",
    "                                    new_min, new_max),\n",
    "                      new_min, new_max)\n",
    "              for b in bands]\n",
    "    return scaled\n",
    "    \n",
    "def bands_to_display(bands, scale=True, alpha=False):\n",
    "    \"\"\"Converts a list of 3 bands to a 3-band rgb, normalized array for display.\"\"\"  \n",
    "    assert len(bands) in [1,3]\n",
    "    all_bands = np.dstack(bands)\n",
    "    \n",
    "    if scale:\n",
    "        scaled = \n",
    "    old_min = np.percentile(all_bands, 2)\n",
    "    old_max = np.percentile(all_bands, 98)\n",
    "    old_min = all_bands.min()\n",
    "    old_max = all_bands.max()\n",
    "\n",
    "    new_min = 0\n",
    "    new_max = 1\n",
    "    scaled = [np.clip(_linear_scale(b.astype(np.float),\n",
    "                                    old_min, old_max,\n",
    "                                    new_min, new_max),\n",
    "                      new_min, new_max)\n",
    "              for b in bands]\n",
    "\n",
    "    filled = [b.filled(fill_value=new_min) for b in scaled]\n",
    "    if alpha:\n",
    "        filled.append(_mask_to_alpha(scaled))\n",
    "\n",
    "    return np.dstack(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295368/1000000 (29.5368%) masked\n",
      "(0.067990905871377599, 0.36307512027914396)\n",
      "(0.061612413226528003, 0.35306403262990721)\n",
      "(0.037686084291491999, 0.36434713031043736)\n",
      "(0.14062722723785701, 0.49724242437433497)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAFoCAYAAABKXw0FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS5IkSSUGZEmSJKnEgCxJkiSV/H9ILWj0TBlntQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b71adedd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_rgb(named_bands):\n",
    "    '''convenience function to aid in displaying a NamedBands image'''\n",
    "    return [named_bands.r, named_bands.g, named_bands.b]\n",
    "\n",
    "def show_bands(bands):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(10,10))\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_adjustable('box-forced')\n",
    "    \n",
    "    rgb_bands = get_rgb(bands)\n",
    "    ax1.imshow(bands_to_display(rgb_bands, alpha=False))\n",
    "    ax1.set_title('Reflectance Bands, No Alpha')\n",
    "\n",
    "    ax2.imshow(bands_to_display(rgb_bands, alpha=True))\n",
    "    ax2.set_title('Reflectance Bands, Alpha from Mask')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def _test():\n",
    "    def _read_window(filename, window):\n",
    "        with rasterio.open(filename, 'r') as src:\n",
    "            print('hi')\n",
    "            return src.read(window=window)\n",
    "\n",
    "    window = ((500,1500),(500,1500))\n",
    "#     window = ((0,4000),(0,4000))\n",
    "    window = ((3000,4000),(3000,4000))\n",
    "    refl_bands = load_reflectance_bands(train_files['scene'],\n",
    "                                        train_files['udm'],\n",
    "                                        train_files['metadata'],\n",
    "                                        window=window)\n",
    "    print(check_mask(refl_bands))\n",
    "    for b in refl_bands:\n",
    "        print (b.min(), b.max()) \n",
    "    show_bands(refl_bands)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features from Image\n",
    "\n",
    "We calculate the classification features from the scene and convert them to an array for feeding into the CART classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndvi(bands):\n",
    "    ndvi = (bands.nir.astype(np.float) - bands.r) / (bands.nir + bands.r)\n",
    "    return ndvi\n",
    "\n",
    "\n",
    "def build_feature_bands(bands):\n",
    "    \"\"\"Prepare bands representing pixel features and provide feature names.\n",
    "    \n",
    "    Takes as input NamedBands\n",
    "    Returns (1) tuple of bands representing features and (2) feature names\n",
    "    \"\"\"\n",
    "    # not copying these bands to minimize memory footprints\n",
    "    features = (bands.b, bands.g, bands.r, bands.nir, calculate_ndvi(bands))\n",
    "    feature_names = ('Blue', 'Green', 'Red', 'NIR', 'NDVI')\n",
    "    return features, feature_names\n",
    "\n",
    "\n",
    "def display_feature_bands(bands, names):\n",
    "    # for this notebook, we know there are 4 features and we will use that\n",
    "    # knowledge to side-step some logic in organizing subplots\n",
    "    assert len(bands) == 5 \n",
    "    \n",
    "    fig, subplot_axes = plt.subplots(nrows=3, ncols=2,\n",
    "                                     sharex=True, sharey=True,\n",
    "                                     figsize=(10,10))\n",
    "    axes = subplot_axes.flat[:-1]\n",
    "    delaxis = subplot_axes.flat[-1]\n",
    "    fig.delaxes(delaxis)\n",
    "    for ax, band, name in zip(axes, bands, names):\n",
    "        ax.set_adjustable('box-forced')\n",
    "        ax.axis('off')\n",
    "\n",
    "        pcm = ax.imshow(band, alpha=True)\n",
    "        ax.set_title(name)\n",
    "        fig.colorbar(pcm, ax=ax,\n",
    "                     pad=0.05, shrink=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def _test():\n",
    "    refl_bands = load_reflectance_bands(train_files['scene'],\n",
    "                                        train_files['udm'],\n",
    "                                        train_files['metadata'],\n",
    "                                        window=((500,1500),(500,1500)))\n",
    "    feat_bands, feat_names = build_feature_bands(refl_bands)\n",
    "    display_feature_bands(feat_bands, feat_names)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scales on the colorbars, we see that the scales of the features are not all the same. 'Blue' feature max is ~0.35 while NDVI feature max is ~0.9. This is not a problem for decision trees, which do not require that features be normalized before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_X(feature_bands):\n",
    "    \"\"\"Convert feature_bands (tuple of bands) to 2d array for working with classifier.\n",
    "    \"\"\"\n",
    "    return np.stack([f.compressed() for f in feature_bands], # exclude masked pixels\n",
    "                     axis=1)\n",
    "\n",
    "\n",
    "def _test():\n",
    "    refl_bands = load_reflectance_bands(train_files['scene'],\n",
    "                                        train_files['udm'],\n",
    "                                        train_files['metadata'],\n",
    "                                        window=((500,1500),(500,1500)))\n",
    "    feat_bands, feat_names = build_feature_bands(refl_bands)\n",
    "    \n",
    "    X = to_X(feat_bands)\n",
    "    print(X.shape)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Standard Dataset to Labels\n",
    "\n",
    "#### Load and Visualize Gold Classified Band\n",
    "\n",
    "Filter to corn and soybean classes and visualize the classified band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class labels from metadata\n",
    "CLASS_LABELS = {1: 'corn', 5: 'soybeans'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_class_band(gold_filename, class_labels=None, window=None, fill_value=0):\n",
    "    gold_class_band = _read_window(gold_filename, window)[0,...]\n",
    "    \n",
    "    try:\n",
    "        # mask pixels with a value not in class_labels\n",
    "        masks = [gold_class_band == val for val in class_labels.keys()]\n",
    "        mask = np.any(np.dstack(masks), axis=2)\n",
    "        mask = ~mask\n",
    "        \n",
    "        masked_band = np.ma.array(np.ma.array(gold_class_band, mask=mask).filled(fill_value=fill_value),\n",
    "                                  mask=mask)\n",
    "    except AttributeError:\n",
    "        # mask nothing\n",
    "        null_mask = np.zeros(gold_class_band.shape, dtype=np.bool)\n",
    "        masked_band = np.ma.array(gold_class_band, mask=null_mask)\n",
    "\n",
    "    return masked_band\n",
    "\n",
    "def _test():\n",
    "    print np.unique(load_gold_class_band(train_files['gold']))\n",
    "    print np.unique(load_gold_class_band(train_files['gold'], class_labels=CLASS_LABELS))\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _imshow_class_band(ax, class_band, class_labels=None, cmap='viridis'):\n",
    "    \"\"\"Show classified band with legend. Alters ax in place.\n",
    "    \n",
    "    possible cmaps ref: https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    \"\"\"\n",
    "    im = ax.imshow(class_band, cmap=cmap)\n",
    "\n",
    "    try:\n",
    "        # add class label legend\n",
    "        # https://stackoverflow.com/questions/25482876/how-to-add-legend-to-imshow-in-matplotlib\n",
    "        colors = [im.cmap(im.norm(value))\n",
    "                  for value in class_labels.keys()]\n",
    "        labels = class_labels.values()\n",
    "\n",
    "        # https://matplotlib.org/users/legend_guide.html\n",
    "        # tag: #creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
    "        patches = [mpatches.Patch(color=c, label=l) for c,l in zip(colors, labels)]\n",
    "\n",
    "        ax.legend(handles=patches, bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0.)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "def plot_classified_band(class_band, class_labels=None, cmap=None, title='Class Labels', figdim=10):\n",
    "    fig = plt.figure(figsize=(figdim, figdim))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    _imshow_class_band(ax, class_band, class_labels, cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def _test():\n",
    "    plot_classified_band(load_gold_class_band(train_files['gold'],\n",
    "                                              class_labels=CLASS_LABELS,\n",
    "                                              window=((500,1500),(500,1500))),\n",
    "                         class_labels=CLASS_LABELS, title='Gold Class Labels, Windowed', figdim=5)\n",
    "\n",
    "    plot_classified_band(load_gold_class_band(train_files['gold'], class_labels=CLASS_LABELS),\n",
    "                         class_labels=CLASS_LABELS, title='Gold Class Labels')\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_y(classified_band):\n",
    "    return classified_band.compressed()\n",
    "\n",
    "def _test():\n",
    "    print(to_y(load_gold_class_band(train_files['gold'], class_labels=CLASS_LABELS)).shape)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Classifier\n",
    "\n",
    "Right now we are just going to train the classifier with a subset of data from the Orthotile (the window we have been working with thus far) and use the dummy label data generated above. The goal is to ensure we can run through training and predicting with the classifier and end up with a classified band that looks like the original in shape/mask and shows the classes as labeled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = ((500,1500),(500,1500))\n",
    "window = None\n",
    "window = ((0,2000),(0,2000))\n",
    "window = ((0,4000),(0,4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refl_bands = load_reflectance_bands(train_files['scene'],\n",
    "                                    train_files['udm'],\n",
    "                                    train_files['metadata'],\n",
    "                                    window=window)\n",
    "feat_bands, _ = build_feature_bands(refl_bands)\n",
    "X = to_X(refl_bands)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_class_band = load_gold_class_band(train_files['gold'], class_labels=CLASS_LABELS, window=window)\n",
    "gold_class_band.mask = get_mask(feat_bands)\n",
    "y = to_y(gold_class_band)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unconstrained, decision trees overfit the training data. Therefore, we must constrain the decision tree by setting a hyperparameter. For now, we will just blindly set the `max_depth` hyperparameter. Eventually, it would be ideal to use cross-validation to identify the best hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see what classes are predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)\n",
    "print(np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classified_band_from_y(y, band_mask):\n",
    "    class_band = np.ma.array(np.zeros(band_mask.shape),\n",
    "                             mask=band_mask.copy())\n",
    "    class_band[~class_band.mask] = y\n",
    "    return class_band\n",
    "\n",
    "pred_band = classified_band_from_y(y_pred, get_mask(refl_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_truth(predicted_class_band, gold_class_band, class_labels=None, figsize=(15,15)):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
    "                                   sharex=True, sharey=True,\n",
    "                                   figsize=figsize)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "    _imshow_class_band(ax1, predicted_class_band, class_labels=class_labels)\n",
    "    ax1.set_title('Classifier Predicted Classes')\n",
    "\n",
    "    _imshow_class_band(ax2, gold_class_band, class_labels=class_labels)\n",
    "    ax2.set_title('Gold Dataset Classes')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_predicted_vs_truth(pred_band, gold_class_band,class_labels=CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! This looks promising. The predicted classes just look like a higher-resolution version of the gold dataset classes. This makes sense, the gold dataset image was upsampled from 30m to 3m (the PS Orthotile pixel size) using nearest-neighbor so would not have the detail to match a prediction performed on the PS Orthotile resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
